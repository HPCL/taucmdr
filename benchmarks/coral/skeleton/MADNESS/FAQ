Vendor questions have been downsampled so as to obscure their 
origin.  The answers are less filtered.

As an overarching response to questions about modifications,
the data provided in the vendor response should contain all of the code, 
verbose output indicating how many simultaneous mTxm calls were made at a time, 
the arguments for all calls (or a hash key to reproduce them) in the test,
and the aggregrate performance results for each problem size in the set.

We still want the sequential result for reference but it won't be used as the FOM.  
We just want the most detailed picture of how mTxm performs so that we can 
estimate the total application performance since this is, after all, 
the point of these benchmarks.

Q1

Can we batch different sized GEMMs together as one kernel and then 
call this kernel many times to find the fastest execution time?

That is, can we transform this:

for (ni=2; ni<60; ni+=2) 
    timer("(m*m)T*(m*m)", ni,ni,ni,a,b,c);

To this:

for (loop=0; loop<30; ++loop)
     FusedKernel(2,60,2,a,b,c);

A1

No, unfortunately that's not the point of the benchmark.  We are 
trying to understand the performance of single execution unit 
(whatever the smallest independently controllable unit is) 
with this benchmark because that is the smallest building block we 
can identify that is flop-intensive.  
Running the parameter sweep in parallel obfuscates the thing 
we're looking for here.

However, if you want, you can run the same kernel N times across the 
whole "node" and divide by N if that seems more sensible for the 
hardware in question.  However, you need to include the aggregate time 
including a start/stop barrier and the kernel invocation must be 
compatible with the launching of N independent, possible different, 
tasks.

For example:

/* for (my_pe_id=0; my_pe_id<num_pes; my_pe_id++)
 *   allocate and initialize a[my_pe_id],b[my_pe_id],c[my_pe_id]; */

for (ni=2; ni<60; ni+=2) {
    double t0 = gettime();
    PE_Barrier(num_pes);
    if (my_pe_id==0)
        FatKernel(ni,ni,ni,a[my_pe_id],b[my_pe_id],c[my_pe_id]);
    else if (my_pe_id==1)
        FatKernel(ni,ni,ni,a[my_pe_id],b[my_pe_id],c[my_pe_id]);
    else if (my_pe_id==2)
        FatKernel(ni,ni,ni,a[my_pe_id],b[my_pe_id],c[my_pe_id]);
    /* etc for all pe ids */
    PE_Barrier(num_pes);
    double t1 = gettime();
    /* compute total flop/s rate of FatKernel per PE as total_flops/(t1-t0)/N  */ 
}

Note that it is essential to have the implementation have divergent 
control aka branching (if I thought someone was going to cheat with a 
magic compiler, I would saw that FatKernel has to be replaced with 
function pointer table with a uniquely compiler instance of the kernel 
for each SM) since we are going to call mTxm as part of a task 
parallel programming model and we cannot have the test run in such a 
way that this would not be possible.  See the other test for how this 
occurs with Pthreads and TBB.  Basically, what we are doing is as if 
you replaced the daxpy call in the other test with mTxm.

